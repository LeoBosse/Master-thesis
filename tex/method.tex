%% ================================================================================
\chapter{Method}
\label{ch:method}
%% ================================================================================

The following chapter will present the the general method and the tools used through the internship.

\section{Data origin}
\label{sec:Data_origin}
%	-Data origin
%		-Fermi collaboration
%		-LAT
%		-Fermi FTOOLS
%		-point source subtraction
%		-pass7 vs pass8
%			-wider energy range
%			-better statistical and systematic errors
%			-better event selection


\begin{figure}[h]
  \centering
  \begin{minipage}[h]{0.45\textwidth}
  	\centering
	\includegraphics[width=1.\linewidth]{pic/method/Flux_FermiData_raw_E12.png}
  	\subcaption{Raw data flux from the LAT}
  	\label{fig:raw_data}
  \end{minipage}
  \hfill
  \begin{minipage}[h]{0.45\textwidth}
	  \centering
	  \includegraphics[width=1.\linewidth]{pic/method/Flux_FermiData-3FGL_E12.png}
	  \subcaption{Raw data with point sources fromn 3FGL catalog subtracted.}
	  \label{fig:data_ptsrc_subtracted}
  \end{minipage}
  \caption{Figures of data before and after point source subtraction. (Maybe also some spectrum with pass7 and pass8 to see the differences.)}
  \label{fig:method_pass8} 
\end{figure}

\begin{figure}
 \centering
 \includegraphics[width=.9\linewidth]{pic/dummy.png}
 \caption{Images of the exposure map}
 \label{fig:proton_spec}
\end{figure}


First of all, all the following work is based on the measurement of gamma rays coming from the milky way and beyond. The quality and accuracy of the data points is one of the most important point that will determine the general quality of the results. Thus it is capital to be sure that the gathering and treatment of the data was done properly.

The data used in the next chapters was observed by the Fermi collaboration Large Area Telescope (LAT) between 2008 and today. They are available on the web and can be treated by anybody using the software given by Fermi.\\

Since the observation is still on-going, it is important to stay up-to-date. The treatment method of the data is also being improved regularly, resulting, among others, in better statistics, better systematic errors, wider energy range and better point source subtraction.
One of the most important step in the treatment process is the first one, the selection of events. Every photon measured is saved along with all its properties in a big data file. Then this long list has to be filtered to keep only the interesting observations. The filter can be based on the incoming direction, the energy or the time of observation, but also on the quality of the event reconstruction. This last cut can be critical. It will determine if the chances that the measured event is in fact a gamma ray, and not some background noise polluting the data. Of course, the more strict is the filter, the less events are kept for analysis and the statistical errors can become large. This work uses the CLEAN class recommended by the Fermi team for diffuse analysis.

The details of the selection can be found in Tab \ref{tab:fermi_selection_parameters}.

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Parameter name} & \textbf{Parameter value} \\ \hline
                        &                          \\ \hline
                        &                          \\ \hline
                        &                          \\ \hline
                        &                          \\ \hline
                        &                          \\ \hline
                        &                          \\ \hline
                        &                          \\ \hline
                        &                          \\ \hline
                        &                          \\ \hline
\end{tabular}
\caption{Parameter values for Fermi data selection}
\label{tab:fermi_selection_parameters}
\end{table}


An other important point is the creation of the exposure map of the telescope. It is basically a map of the time the telescope spend observing a portion of the sky. It is used to correct for an observation bias. If the telescope observed the galactic center ten times more often than the Orion nebula for example, there is no way to know at first if the higher counts in the GC are due to the longer exposition time or a higher incoming intensity. After dividing the count map by the exposure map, a flux map is obtained that does not depends on the observation time of particular regions.


The goal of this work is to study the diffuse sources of gamma-rays from inside and outside the milky way. Of course, the LAT does not differentiate between gamma rays coming from a pulsar or a cosmic ray. This has to be done manually as the last step of the treatment process. A large catalogue of gamma ray point sources (3FGL) is available online on NASA website. This catalogue lists most of the known and identified point sources, along with their spectral shape and flux. These informations can be then used, combined with the exposure map, to model the number of counts coming from point sources, their spatial and energy distribution. Once this model map is done, it is subtracted from the data to keep only the diffuse emission. Of course the models are never perfect and all point sources are not listed. This can lead to errors or anomalies in the observations and this is an other reason to keep the dataset up-to-date.

Once all of the treatment is done, a flux map of the entire sky in $counts/s/m^2/GeV/sr$ is produced. The map is divided in bins of $0.5 \times 0.5$ degrees on a Cartesian projection. Every bin contains 30 logarithmic energy bins ranging from 60 MeV to 513 GeV with 1.2 multiplicative step. The final data cube is therefore of dimension $720 \times 360 \times 30$.

For visibility purposes, every energy bin is multiplied by its energy squared, becoming an energy flux in $GeV/s/m^2/sr$.

The errors on the data are come from two sources. The first are systematic errors due to the instrument or the treatment process. They are pretty low, around 3\%, but can change for low or high energies as seen in Fig. \ref{fig:LAT_sys_err}. The other source is the statistical errors, simply proportional to the square root of counts (N). The final equation is given as follow :

\begin{equation}
\sigma_{tot} =\sqrt{\sigma_{sys}^2 + \sigma_{stat}^2} = \sqrt{\sigma_{sys}^2 + \frac{1}{N}}
\end{equation}


\begin{figure}[h]
 \centering
 \includegraphics[width=.9\linewidth]{pic/method/LAT_sys_error.png}
 \caption{Systematic error for Pass 8 data.}
 \label{fig:LAT_sys_err}
\end{figure}


\section{Model components}
\subsection{Basic components}
%	-3 Basic components
%		-PCR
%			-proton CR follow power-law E^-2.849
%			?-try with break at 5-10GeV with index ~ -2.7 ?
%		-IC and BR
%			-electron CR spectrum E^-3.21
%			-break at 1GeV, index E^-3.21 + 2.4 below break


\subsubsection{$\pi^0$ production by propagated cosmic rays (PCR)}

\begin{figure}
 \centering
 \includegraphics[width=.9\linewidth]{pic/dummy.png}
 \caption{images for proton spectrum for PCR, MCR, SCR, data}
 \label{fig:proton_spec}
\end{figure}

%%% ARTICLE
The initial propagated proton spectrum for the PCR template is obtained from the locally observed proton data from AMS-02 \todo{cite[55]}. A good approximation is an unbroken power law ($R-\alpha$) with a spectral index ($\alpha$) of 2.85 at rigidities above 45 GV. At lower rigidities the data are below the power law because of solar modulation \todo{cite[56]}, as can be seen from Fig. \ref{fig:proton_spec}, where the AMS-02 data are plotted as well. To find the best parametrization a set of broken power laws with a grid of breaks and spectral indices above and below the break was constructed and the optimal parametrization was found by interpolation between the fits with the best test statistic. 
The gamma-ray data are well described by an unbroken power law for the protons with a spectral index ($\alpha$) of 2.85 at all rigidities.\\
%%% ARTICLE

There is also the possibility to introduce a break around a few GeV, to have a slightly harder spectrum at lower energies.\\


\subsubsection{Inverse compton (IC) and bremmstrahlung (BR)}

\begin{figure}
 \centering
 \includegraphics[width=.9\linewidth]{pic/dummy.png}
 \caption{images for electron spectrum for IC, BR, data}
 \label{fig:electron_spec}
\end{figure}
\begin{figure}
 \centering
 \includegraphics[width=.9\linewidth]{pic/dummy.png}
 \caption{images IC and BR variations over the sky}
 \label{fig:el_variations}
\end{figure}

%%% ARTICLE
The interstellar electron spectra needed a break around \todo{1 GeV} with a spectral index of 3.21 above the break, which is compatible with the locally observed electron spectrum (see Fig. \ref{fig:electron_spec}). Below the break the optimal spectral index is 0.81, which implies a suppression of electrons. The break point might be related to the fact that around 1 GeV electrons have the smallest energy losses, since above this energy synchrotron, BR and IC dominate the energy losses, while below this energy ionization losses become strong, thus depleting the electron spectrum below 1 GeV. A similar break in the electron spectrum was needed in the Fermi diffuse model \todo{cite[46]}.\\
The targets for the production of gamma-rays are the interstellar gas and the interstellar radiation field, which are both stronngly dependant of position, %The latter consists of photons from the cosmic microwave background, the infrared radiation from hot matter, like dust and the star light
, so the photon composition varies with sky direction. 
Hence, for the IC templates we have to calculate the templates for each sky direction. The variation over the sky is about $\pm 10\%$, as shown in \ref{fig:el_variations}; for the BR template the spread is considerably smaller, as shown in \ref{fig:el_variations}. %Note that the intensity of photons in the interstellar radiation field nor the gas density play any role in a template analysis, since the intensity of each contribution to the gamma-ray sky is determined by the fitted normalization factors in Eq. 1.
%%% ARTICLE


%The IC and BR spectra are both obtained from the interstallar electron distribution, taken here as a broken powerlaw with a break \todo{at 1GeV}. The index is 3.21 over the break and 0.8 below.
%The IRF is composed in the mostly by starlight (UV), and in with smaller contribution dust emission (IR) and the cosmic microwave background. The first two components of the IRF are position dependant, and this is why we have to calculate the IC spectrum for every sky direction.
%The BR component is directly linked to the gas, and charged particles distribution. This is also a reason too calculate the BR component in every sky directions. The variations are not too large.




\subsection{Additional components}
%	-2 or more additional components:
%		-SCR
%			-proton CR follow power-law E^-2.1
%		-MCR
%			-proton CR follow power-law E^-2.849
%			-break between 6 and 14GeV, index E^-2.849 + 2.149 below break
%		-DM
%			-Dark Susy
%			-Determination of Mass using best fit in CMZ
%		-MBR
%			-electron CR spectrum E^-3.21
%			-break between 6 and 14 GeV, index E^-3.21 + 2.4 below break
%	-Isosky
%		-Calculated from fermi model and adjusted in our fit

\begin{figure}
 \centering
 \includegraphics[width=.9\linewidth]{pic/dummy.png}
 \caption{images of MCR, MSP, DM on single plot.}
 \label{fig:excess_component_comp}
\end{figure}

\subsubsection{SCR component}

%%% ARTICLE
The proton spectra for the SCR template can be described by an unbroken power law with a spectral index of 2.1, as obtained from the best fit. The index 2.1 for the SCR template agrees with the data from the Fermi Bubbles, shown by the data points inside the shaded band in Fig. \ref{fig:proton_spec}; the index 2.1 is expected from diffuse shock wave acceleration. \todo{cite[60, 61]} The fact that the Fermi Bubbles and the cosmic rays inside sources have the same spectrum strongly suggests that they are connected by point sources providing advective outflows of gas in the Galactic center. \todo{cite[44]}
%%% ARTICLE

%The proton spectra for the SCR component is a single powerlaw of index 2.1 expecteed from diffuse schock acceleration.

\subsubsection{MCR component}
%%% ARTICLE
The decreasing gamma-ray emissivity from MCs below 2 GeV could be parametrized by a break in the power law of the corresponding proton spectrum. Above the break the optimal spectral index of 2.85 was found to be the same as for the PCR spectrum, as expected if the high energy propagated protons are above a certain magnetic cutoff. But below the break, which varies according to the fit from 6 to 14 GV for the different clouds, the optimal spectral index is 0.7, thus providing a significant suppression of protons below the break, as can be seen from Fig. \ref{fig:electron_spec}. Energy losses alone cannot reproduce such a suppression of the proton spectrum below the break, but magnetic cutoffs are able to do so. Such a cutoff is well known from cosmic rays entering the Earth's magnetic field: particles below typically 20 GV entering near the magnetic equator do not reach the Earth, but are repelled into outer space by the geomagnetic cutoff. \todo{cite[62]} The rigidity cutoff of 20 GV is proportional to the magnetic moment. Although the magnetic field near the Earth (0.5 G) is orders of magnitude higher than the typical magnetic fields in dense MCs \todo{cite[63]}, the much larger sizes of MCs - or its substructure of filaments and cloudlets \todo{cite[64]} - yield magnetic moments of the same order of magnitude as the Earth's magnetic moment, so similar magnetic cutoffs are plausible. Variations in the magnetic cutoff in MCs are expected from the variations in size and in magnetic field; the latter increases with MC density. \todo{cite[63]} The variations of the break in the proton spectrum between 6 and 14 GV varies the maximum of the gamma-ray spectrum from 2 to 1 GeV\todo{??, as shown in Fig. ref??}.
The fit prefers a constant spectral index below the break for all sky directions. Such a constant spectral index is plausible with regular magnetic fields oriented in the disk \todo{cite[65, 66]} and the "cloudlets" inside MCs \todo{cite[64]} form magnetic dipole fields. Then the maximum cutoff occurs for cosmic rays entering from the halo perpendicular into the cloud for any orientation of the magnetic dipole. For a given entrance angle the cutoff would provide a sharp break, but for an isotropic distribution of entrance angles the break points are smeared. A distribution of break points will provide a slope below the maximum break determined largely by the isotropic distribution of the entrance angles into the disk. Since this distribution is the same for all MCs the slopes below the break will be similar for all MCs, even if the maximum break (= maximum magnetic cutoff) varies.
%%% ARTICLE

%
%The MCR component is produced by the same process than for PCR, but with a different proton distribution. Due to the magnetic cut-off in MCs, we introduce a break in the proton spectrum between 6 and 14GeV. We leave the index above to 2.85 as for PCR as expected, but we reduce it to 0.7.\\
%
%The magnetic fields around such clouds are not as strong than what we have in the solar system, but the spatial scales are much bigger and could produce such a break. Of course it position may vary from clouds to clouds and it is why we choose to free its position when fitting.\\
%
%This gives us a spectrum peaking around 2GeV.


\subsubsection{Dark matter (DM)}
%%% ARTICLE
DM particles are expected to annihilate and just like in electron-positron annihilation the annihilation energy of
roughly twice the WIMP mass will lead to the production of hadrons, thus producing copiously gamma-rays from $\pi^0$ decays. A smaller fraction of WIMP annihilation is expected to lead to tau lepton pairs, which can lead to $\pi^0$ production in the hadronic tau decays. This contribution is expected to be small and is neglected. The DM template can be calculated with DarkSusy. \todo{cite[67, 68]} The annihilation signal peaking at 2-3 GeV requires a WIMP mass around 45 GeV, as shown in Fig. \ref{fig:excess_component_comp} as well. The difference to the MCR template is the cutoff at twice the WIMP mass, which is absent in the MCR template.
%%% ARTICLE

%
%The DM spectrum is calculated using the DarkSUSY software. Since the initial WIMP mass is not known, we chose to let the fit decide in the CMZ region and fix it to this value in the other regions. The value generally turns around 50GeV, which make the gamma-ray spectrum peak around 2GeV.


\subsubsection{Milli-second pulsars (MSP)}

The MSP spectrum is directly taken from the Fermi study \todo{cite Fermi}

\subsubsection{Isitropic component}

\begin{figure}
 \centering
 \includegraphics[width=.9\linewidth]{pic/dummy.png}
 \caption{Images to illustrate isotropic calibration}
 \label{fig:iso_calibration}
\end{figure}

%%% ARTICLE
The isotropic template represents the contribution from the isotropic extragalactic background and hadron misiden-tification.  Its spectral shape and absolute normalization are provided within the Fermi software. \todo{cite[51]} The isotropic template was redetermined for our analysis in the following way.\\
We fit the data in regions outside the Bubbles and Galactic disk using the isotropic template from the Fermi software as an initial estimate in the fit. If one plots the total observed gamma-ray flux versus the fitted flux in the various cones in a certain energy bin, one expects a linear relation crossing the origin, if the isotropic flux is estimated correctly. However, if there is a missing or too high isotropic contribution, this leads to an offset at the origin of the linear curve, since the isotropic component is by definition the same for all cones, so it shifts the whole curve up and down for each energy bin. An example of such a fit is shown in Figure \ref{fig:iso_calibration} for an energy bin between \todo{3.7-5.2 GeV}. The offset can be determined for each energy bin, which yields the spectral template of the isotropic component. The final spectral template is obtained by iteration until zero offset at the origin is reached. The resulting template in our analysis has deviations from the Fermi template up to $35\%$ above 2 GeV, as shown in the insert of \ref{fig:iso_calibration}.
%%% ARTICLE

\section{Fitting method}
%My method:
%	-Spectral templates fitted to the data
%		-independant spatial cones on the entire sky (usually 797 for optimized sizes)
%		-minimum chi2 fit using ROOT for every cone
%		-Benefits
%			-energy related features
%			-only a few degrees of freedom -> Well constrained fit (5(or 6) dof against 21-30 points)
%		-Downside:
%			-No spatial templates. (only the isosky)


The fitted data can be seen as a data cube whose dimension are longitude, latitude and energy. The finest spatial grid is divided in $720 \times 360$ cones of $ 0.5^\circ \times 0.5^\circ $. Every cone contains 30 energy bins. This allows to treat different portions of the sky independently of one another. Since the cones do not have the same solid angle and the statistic in a small bining can be problematic, the grid used is more often composed of 797 cones of different sizes, bigger at the poles or and smaller near the equator. This allows a better statistic in lower flux regions at high latitudes and where the resolution is less important. In the same time, the equator and the GC have a lot more counts and can be treated in a smaller binning. It is also way faster to compute.\\

%The model uses a certain number of components (from three to six) each corresponding to a certain phenomenon. They all have a certain energy spectra, that can vary with the position in the sky (for PCR, BR and IC).
The fits are done in the following way for every bin independantly. After choosing the templates used for the fit, their scaling factor is only degree of freedom allowed. Using a ROOT TVirtualFitter object, every template is scaled up or down until their sum comes the closest to the data. 
Mathematically, the minimum distance between the model and the data is found when the $\chi^2$ value is lowest. It is calculated as follows:

\begin{equation}
\chi^2 = \sum_{i=1}^{30}[\frac{(D_i - \sum_{j=1}^{n}n_j.T_{ij})^2}{\sigma^2}]
\end{equation}

where:
\begin{itemize}
\item $D_i$ is the data flux in the $i^th$ energy bin.
\item $n_j$ is the scaling factor for the $j^th$ model component.
\item $T_{ij}$ is the model flux of the $j^th$ in the $i^th$ energy bin.
\item $\sigma_i$ is the geometric mean of the statistical and systematical error of the Fermi data point $i$.
\end{itemize}

The fit is very well constrained with only five or six degrees of freedoms depending on the fit against 30 data points. A usefull value is the $\chi^2 / d.o.f$ where $d.o.f = \#data\ points - \#degrees\ of\ freedom - 1$. This rescaling has the advantage to bring the perfect $\chi^2$ value down to one, thus making the comparison between different fits easier. This rescaling will be applied everytime when speaking about $\chi^2$ in the rest of the discussion, except if explicitly told. 
The closest a $\chi^2$ value is to one, the better the model follows the data. The higher it gets, the lower the quality of the fit. It can also happen that it gets lower than one. This can happen when the fit is good, but the error bars on the data are too big.

On the other hand, it is not possible to implement a spatial template where the spatial shape of a component would be fixed in advance. For example a component with a spherical distribution around the GC. The hope is to let the fit find reasonnable shapes by itself only using the $\chi ^2$ minimization.\\

It allows to study at the same time the different contributions in one portion of the sky, and the skymaps of every component.\\



\section{Introduction of results}

Recretion of previous studies (GC excess, problems).\\
Introduction of new component to take care of those problems (SCR for high energies, MCR, Dm or MSP for 2GeV excess)


%%	-Weniger plots
%%		-study of spectra slope between 0.3 and 2 GeV
%%	-Specklings
%%		-Study of symmetry
%%	-Comparison with CO map
